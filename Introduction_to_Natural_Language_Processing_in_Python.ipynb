{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to Natural Language Processing in Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcK8PhZqVPEe"
      },
      "source": [
        "grail = \"https://assets.datacamp.com/production/repositories/932/datasets/4921d0bf6a73fd645f49f528faf74a871bb3a0e9/grail.txt\"\n",
        "english_stopwords = \"https://assets.datacamp.com/production/repositories/932/datasets/8042ed46ae7faef4951fcda771c5acc4fc3c0bf6/english_stopwords.txt\"\n",
        "debugging = 'https://raw.githubusercontent.com/Andikazidanef15/Introduction-to-NLP/main/wiki_text_debugging.txt'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg_S6U3LW1c5"
      },
      "source": [
        "import urllib.request\n",
        "content=urllib.request.urlopen(grail)  \n",
        "content_1 = urllib.request.urlopen(debugging)\n",
        "holy_grail = content.read().decode('utf-8')\n",
        "article = content_1.read().decode('utf-8')\n",
        "\n",
        "content.close()\n",
        "content_1.close()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxGugau4ZveA"
      },
      "source": [
        "# Regular Expression and Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxYehOfcZ6-Z"
      },
      "source": [
        "import re"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB3Lq_6uZ0pA",
        "outputId": "d0cf267d-b593-4d4b-ffdc-1f89b0d96548"
      },
      "source": [
        "my_string = \"Let's write RegEx!\"\n",
        "re.findall(r\"\\w\",my_string)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['L', 'e', 't', 's', 'w', 'r', 'i', 't', 'e', 'R', 'e', 'g', 'E', 'x']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nJrJ-eCZ9u6",
        "outputId": "65fc84ab-c727-485e-890e-cfa11db5aacb"
      },
      "source": [
        "re.findall(r\"\\w+\",my_string)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let', 's', 'write', 'RegEx']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6rsaycqaDye",
        "outputId": "0af846f9-8b94-42aa-920d-7a318bf7ea72"
      },
      "source": [
        "re.findall(r\"\\s+\",my_string)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ', ' ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCzpiRMuYNfX"
      },
      "source": [
        "## Practicing regular expressions: re.split() and re.findall()\n",
        "Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps.\n",
        "\n",
        "Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, \"\\n\" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string \"\\n\" - that is, the character \"\\\" followed by the character \"n\" - and not as a new line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlWh49RVaNMl",
        "outputId": "464e18bd-c7c5-49b2-9d2e-a02b7cd121a9"
      },
      "source": [
        "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
        "\n",
        "# Write a pattern to match sentence endings: sentence_endings\n",
        "sentence_endings = r\"[.?!]\"\n",
        "\n",
        "# Split my_string on sentence endings and print the result\n",
        "print(re.split(sentence_endings, my_string))\n",
        "\n",
        "# Find all capitalized words in my_string and print the result\n",
        "capitalized_words = r\"[A-Z]\\w+\"\n",
        "print(re.findall(capitalized_words, my_string))\n",
        "\n",
        "# Split my_string on spaces and print the result\n",
        "spaces = r\"\\s+\"\n",
        "print(re.split(spaces, my_string))\n",
        "\n",
        "# Find all digits in my_string and print the result\n",
        "digits = r\"\\d+\"\n",
        "print(re.findall(digits, my_string))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
            "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
            "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
            "['4', '19']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlWXlwrjPafJ"
      },
      "source": [
        "## Word Tokenization with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtaogHJlQVe-"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wzpa1bCGQSFm",
        "outputId": "438df0d5-cefd-4d05-c223-7e11ffbfe9a3"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-1xHQO1Pcq4"
      },
      "source": [
        "scene_one = \"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\"\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xsB5NNpQOX5",
        "outputId": "532f30e8-65fa-4fc0-8c7c-8f6e777e460a"
      },
      "source": [
        "# Import necessary modules\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Split scene_one into sentences: sentences\n",
        "sentences = sent_tokenize(scene_one)\n",
        "\n",
        "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[3])\n",
        "\n",
        "# Make a set of unique tokens in the entire scene: unique_tokens\n",
        "unique_tokens = set(word_tokenize(scene_one))\n",
        "\n",
        "# Print the unique tokens result\n",
        "print(unique_tokens)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ridden', 'there', 'using', 'KING', 'that', 'an', 'order', 'course', 'carry', '[', 'join', 'speak', 'you', 'are', 'every', 'pound', '?', '...', 'two', 'Well', 'migrate', 'covered', 'climes', 'grip', 'land', 'ratios', 'sovereign', 'Pendragon', 'and', 'breadth', 'suggesting', 'matter', 'not', 'anyway', 'who', 'right', 'ounce', \"'\", 'Patsy', 'by', 'But', 'sun', 'beat', 'it', 'tropical', 'husk', 'seek', 'all', 'SCENE', 'with', 'horse', 'from', 'under', 'trusty', '--', 'I', 'Yes', 'a', 'Will', 'knights', 'swallow', 'times', 'carrying', 'Court', 'wants', \"'m\", 'get', 'Whoa', 'guiding', 'A', 'its', 'So', 'Please', 'here', 'It', 'Supposing', 'on', '!', \"'d\", 'minute', \"n't\", 'my', 'empty', 'Am', 'European', \"'em\", 'have', 'winter', 'zone', 'bring', 'am', 'Mercea', 'plover', 'non-migratory', 'agree', 'ask', ']', 'goes', 'in', 'if', 'King', 'length', 'or', 'What', 'Where', 'at', 'The', 'Pull', 'warmer', 'velocity', 'Halt', ',', 'snows', 'halves', 'them', 'to', 'point', 'temperate', 'interested', '#', 'fly', 'creeper', 'Ridden', 'our', 'air-speed', 'may', 'Camelot', 'yet', 'Saxons', 'Who', 'carried', 'second', 'Wait', 'defeator', 'where', 'your', 'he', 'go', 'You', 'coconut', 'bangin', 'back', 'this', 'is', 'together', 'these', 'clop', ':', 'ARTHUR', 'We', 'grips', 'dorsal', 'kingdom', \"'s\", 'wind', 'one', 'five', '1', 'That', '2', 'the', 'question', 'use', 'other', 'me', \"'re\", 'they', 'castle', 'just', 'will', 'then', 'No', 'weight', 'of', 'mean', 'Oh', 'Arthur', 'feathers', \"'ve\", 'yeah', 'maintain', 'Found', 'They', 'In', 'tell', 'needs', 'got', 'SOLDIER', 'forty-three', 'England', 'does', 'simple', 'held', 'south', 'found', 'son', 'through', 'coconuts', 'be', 'line', 'Are', 'could', 'Not', 'maybe', 'but', 'African', 'Uther', 'wings', '.', 'house', 'martin', 'strand', 'strangers', 'do', 'lord', 'must', 'master', 'court', 'why', 'Britons', 'servant', 'bird', 'Listen', 'since', 'search', 'swallows'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAapIVmnQiId"
      },
      "source": [
        "## More regex with re.search()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaH1YjlyQhII",
        "outputId": "f0da1b70-e480-455f-b414-ba631dd4e423"
      },
      "source": [
        "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
        "match = re.search(\"coconuts\", scene_one)\n",
        "\n",
        "# Print the start and end indexes of match\n",
        "print(match.start(), match.end())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "580 588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGihma27RGsW",
        "outputId": "8b381f91-6b22-4915-aa0b-2543fdf03fc1"
      },
      "source": [
        "# Write a regular expression to search for anything in square brackets: pattern1\n",
        "pattern1 = r\"\\[.*]\"\n",
        "\n",
        "# Use re.search to find the first text in square brackets\n",
        "print(re.search(pattern1, scene_one))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBWcYmc8RH-Q",
        "outputId": "9599a540-d911-4886-ed1c-140edb3f7910"
      },
      "source": [
        "# Find the script notation at the beginning of the fourth sentence and print it\n",
        "pattern2 = r\"[A-Z]+:\"\n",
        "print(re.match(pattern2, sentences[3]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_BJxeFEPp-P"
      },
      "source": [
        "## Regex with NLTK tokenization\n",
        "Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer class gives you some extra methods and attributes for parsing tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP7D6qGUPqiA"
      },
      "source": [
        "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
        " '#NLP is super fun! <3 #learning',\n",
        " 'Thanks @datacamp :) #nlp #python']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwPvWGVDP431",
        "outputId": "9e21b3e1-5bf2-46ee-eeb0-e7315bd3dcd7"
      },
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "# Define a regex pattern to find hashtags: pattern1\n",
        "pattern1 = r\"#\\w+\"\n",
        "# Use the pattern on the first tweet in the tweets list\n",
        "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
        "print(hashtags)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#nlp', '#python']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZuNvGNiQBaP",
        "outputId": "56a54a17-ab37-4222-ab71-317477b9ee41"
      },
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "# Write a pattern that matches both mentions (@) and hashtags\n",
        "pattern2 = r\"([\\@|\\#]\\w+)\"\n",
        "# Use the pattern on the last tweet in the tweets list\n",
        "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
        "print(mentions_hashtags)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['@datacamp', '#nlp', '#python']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWWBOwPMQs8r",
        "outputId": "c2806442-651f-480c-bca2-8ede52fae955"
      },
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "# Use the TweetTokenizer to tokenize all tweets into one list\n",
        "tknzr = TweetTokenizer()\n",
        "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
        "print(all_tokens)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-938090Qzkt"
      },
      "source": [
        "## Non-ascii tokenization\n",
        "In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n",
        "\n",
        "Here, you have access to a string called german_text, which has been printed for you in the Shell. Notice the emoji and the German characters!\n",
        "\n",
        "The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize.\n",
        "\n",
        "Unicode ranges for emoji are:\n",
        "\n",
        "('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'), ('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF')."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeHIkP48Q8M-",
        "outputId": "1d3c3321-9418-4fb3-d667-8d86579f3a8e"
      },
      "source": [
        "german_text = 'Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕'\n",
        "\n",
        "# Tokenize and print all words in german_text\n",
        "all_words = word_tokenize(german_text)\n",
        "print(all_words)\n",
        "\n",
        "# Tokenize and print only capital words\n",
        "capital_words = r\"[A-Z\\Ü]\\w+\"\n",
        "print(regexp_tokenize(german_text, capital_words))\n",
        "\n",
        "# Tokenize and print only emoji\n",
        "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
        "print(regexp_tokenize(german_text, emoji))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n",
            "['Wann', 'Pizza', 'Und', 'Über']\n",
            "['🍕', '🚕']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFaFjTjmTZnp"
      },
      "source": [
        "## Charting practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "7CrfNAOgTbai",
        "outputId": "335bd804-c852-4426-bd79-67f9fdc908be"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Split the script into lines: lines\n",
        "lines = holy_grail.split('\\n')\n",
        "\n",
        "# Replace all script lines for speaker\n",
        "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
        "lines = [re.sub(pattern, '', l) for l in lines]\n",
        "\n",
        "# Tokenize each line: tokenized_lines\n",
        "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
        "\n",
        "# Make a frequency list of lengths: line_num_words\n",
        "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
        "\n",
        "# Plot a histogram of the line lengths\n",
        "plt.hist(line_num_words)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA-sVhTDWF_x"
      },
      "source": [
        "# Simple Topic Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBJJfCHTWJS-"
      },
      "source": [
        "## Building a Counter with bag-of-words\n",
        "In this exercise, you'll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as article. Try doing the bag-of-words without looking at the full article text, and guessing what the topic is! If you'd like to peek at the title at the end, we've included it as article_title. Note that this article text has had very little preprocessing from the raw Wikipedia database entry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "I4G9CKX6WIxl",
        "outputId": "49630169-5c7a-49ac-bc46-3f9f7ef82efe"
      },
      "source": [
        "article"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\'\\'\\'Debugging\\'\\'\\' is the process of finding and resolving of defects that prevent correct operation of computer software or a system.  \\n\\nNumerous books have been written about debugging (see below: #Further reading|Further reading), as it involves numerous aspects, including interactive debugging, control flow, integration testing, Logfile|log files, monitoring (Application monitoring|application, System Monitoring|system), memory dumps, Profiling (computer programming)|profiling, Statistical Process Control, and special design tactics to improve detection while simplifying changes.\\n\\nOrigin\\nA computer log entry from the Mark&nbsp;II, with a moth taped to the page\\n\\nThe terms \"bug\" and \"debugging\" are popularly attributed to Admiral Grace Hopper in the 1940s.[http://foldoc.org/Grace+Hopper Grace Hopper]  from FOLDOC While she was working on a Harvard Mark II|Mark II Computer at Harvard University, her associates discovered a moth stuck in a relay and thereby impeding operation, whereupon she remarked that they were \"debugging\" the system. However the term \"bug\" in the meaning of technical error dates back at least to 1878 and Thomas Edison (see software bug for a full discussion), and \"debugging\" seems to have been used as a term in aeronautics before entering the world of computers. Indeed, in an interview Grace Hopper remarked that she was not coining the term{{Citation needed|date=July 2015}}. The moth fit the already existing terminology, so it was saved.  A letter from J. Robert Oppenheimer (director of the WWII atomic bomb \"Manhattan\" project at Los Alamos, NM) used the term in a letter to Dr. Ernest Lawrence at UC Berkeley, dated October 27, 1944,http://bancroft.berkeley.edu/Exhibits/physics/images/bigscience25.jpg regarding the recruitment of additional technical staff.\\n\\nThe Oxford English Dictionary entry for \"debug\" quotes the term \"debugging\" used in reference to airplane engine testing in a 1945 article in the Journal of the Royal Aeronautical Society. An article in \"Airforce\" (June 1945 p.&nbsp;50) also refers to debugging, this time of aircraft cameras.  Hopper\\'s computer bug|bug was found on September 9, 1947. The term was not adopted by computer programmers until the early 1950s.\\nThe seminal article by GillS. Gill, [http://www.jstor.org/stable/98663 The Diagnosis of Mistakes in Programmes on the EDSAC], Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences, Vol. 206, No. 1087 (May 22, 1951), pp. 538-554 in 1951 is the earliest in-depth discussion of programming errors, but it does not use the term \"bug\" or \"debugging\".\\nIn the Association for Computing Machinery|ACM\\'s digital library, the term \"debugging\" is first used in three papers from 1952 ACM National Meetings.Robert V. D. Campbell, [http://portal.acm.org/citation.cfm?id=609784.609786 Evolution of automatic computation], Proceedings of the 1952 ACM national meeting (Pittsburgh), p 29-32, 1952.Alex Orden, [http://portal.acm.org/citation.cfm?id=609784.609793 Solution of systems of linear inequalities on a digital computer], Proceedings of the 1952 ACM national meeting (Pittsburgh), p. 91-95, 1952.Howard B. Demuth, John B. Jackson, Edmund Klein, N. Metropolis, Walter Orvedahl, James H. Richardson, [http://portal.acm.org/citation.cfm?id=800259.808982 MANIAC], Proceedings of the 1952 ACM national meeting (Toronto), p. 13-16 Two of the three use the term in quotation marks.\\nBy 1963 \"debugging\" was a common enough term to be mentioned in passing without explanation on page 1 of the Compatible Time-Sharing System|CTSS manual.[http://www.bitsavers.org/pdf/mit/ctss/CTSS_ProgrammersGuide.pdf The Compatible Time-Sharing System], M.I.T. Press, 1963\\n\\nKidwell\\'s article \\'\\'Stalking the Elusive Computer Bug\\'\\'Peggy Aldrich Kidwell, [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?tp=&arnumber=728224&isnumber=15706 Stalking the Elusive Computer Bug], IEEE Annals of the History of Computing, 1998. discusses the etymology of \"bug\" and \"debug\" in greater detail.\\n\\nScope\\nAs software and electronic systems have become generally more complex, the various common debugging techniques have expanded with more methods to detect anomalies, assess impact, and schedule software patches or full updates to a system. The words \"anomaly\" and \"discrepancy\" can be used, as being more neutral terms, to avoid the words \"error\" and \"defect\" or \"bug\" where there might be an implication that all so-called \\'\\'errors\\'\\', \\'\\'defects\\'\\' or \\'\\'bugs\\'\\' must be fixed (at all costs). Instead, an impact assessment can be made to determine if changes to remove an \\'\\'anomaly\\'\\' (or \\'\\'discrepancy\\'\\') would be cost-effective for the system, or perhaps a scheduled new release might render the change(s) unnecessary. Not all issues are life-critical or mission-critical in a system. Also, it is important to avoid the situation where a change might be more upsetting to users, long-term, than living with the known problem(s) (where the \"cure would be worse than the disease\"). Basing decisions of the acceptability of some anomalies can avoid a culture of a \"zero-defects\" mandate, where people might be tempted to deny the existence of problems so that the result would appear as zero \\'\\'defects\\'\\'. Considering the collateral issues, such as the cost-versus-benefit impact assessment, then broader debugging techniques will expand to determine the frequency of anomalies (how often the same \"bugs\" occur) to help assess their impact to the overall system.\\n\\nTools\\nDebugging on video game consoles is usually done with special hardware such as this Xbox (console)|Xbox debug unit intended for developers.\\n\\nDebugging ranges in complexity from fixing simple errors to performing lengthy and tiresome tasks of data collection, analysis, and scheduling updates.  The debugging skill of the programmer can be a major factor in the ability to debug a problem, but the difficulty of software debugging varies greatly with the complexity of the system, and also depends, to some extent, on the programming language(s) used and the available tools, such as \\'\\'debuggers\\'\\'. Debuggers are software tools which enable the programmer to monitor the execution (computers)|execution of a program, stop it, restart it, set breakpoints, and change values in memory. The term \\'\\'debugger\\'\\' can also refer to the person who is doing the debugging.\\n\\nGenerally, high-level programming languages, such as Java (programming language)|Java, make debugging easier, because they have features such as exception handling that make real sources of erratic behaviour easier to spot. In programming languages such as C (programming language)|C or assembly language|assembly, bugs may cause silent problems such as memory corruption, and it is often difficult to see where the initial problem happened. In those cases, memory debugging|memory debugger tools may be needed.\\n\\nIn certain situations, general purpose software tools that are language specific in nature can be very useful.  These take the form of \\'\\'List of tools for static code analysis|static code analysis tools\\'\\'.  These tools look for a very specific set of known problems, some common and some rare, within the source code.  All such issues detected by these tools would rarely be picked up by a compiler or interpreter, thus they are not syntax checkers, but more semantic checkers.  Some tools claim to be able to detect 300+ unique problems. Both commercial and free tools exist in various languages.  These tools can be extremely useful when checking very large source trees, where it is impractical to do code walkthroughs.  A typical example of a problem detected would be a variable dereference that occurs \\'\\'before\\'\\' the variable is assigned a value.  Another example would be to perform strong type checking when the language does not require such.  Thus, they are better at locating likely errors, versus actual errors.  As a result, these tools have a reputation of false positives.  The old Unix \\'\\'Lint programming tool|lint\\'\\' program is an early example.\\n\\nFor debugging electronic hardware (e.g., computer hardware) as well as low-level software (e.g., BIOSes, device drivers) and firmware, instruments such as oscilloscopes, logic analyzers or in-circuit emulator|in-circuit emulators (ICEs) are often used, alone or in combination.  An ICE may perform many of the typical software debugger\\'s tasks on low-level software and firmware.\\n\\nDebugging process \\nNormally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with Parallel computing|parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.\\n\\nAfter the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, a bug in a compiler can make it Crash (computing)|crash when parsing some large source file. However, after simplification of the test case, only few lines from the original source file can be sufficient to reproduce the same crash. Such simplification can be made manually, using a Divide and conquer algorithm|divide-and-conquer approach. The programmer will try to remove some parts of original test case and check if the problem still exists. When debugging the problem in a Graphical user interface|GUI, the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear.\\n\\nAfter the test case is sufficiently simplified, a programmer can use a debugger tool to examine program states (values of variables, plus the call stack) and track down the origin of the problem(s). Alternatively, Tracing (software)|tracing can be used. In simple cases, tracing is just a few print statements, which output the values of variables at certain points of program execution.{{citation needed|date=February 2016}}\\n\\n Techniques \\n \\'\\'Interactive debugging\\'\\'\\n \\'\\'{{visible anchor|Print debugging}}\\'\\' (or tracing) is the act of watching (live or recorded) trace statements, or print statements, that indicate the flow of execution of a process. This is sometimes called \\'\\'{{visible anchor|printf debugging}}\\'\\', due to the use of the printf function in C. This kind of debugging was turned on by the command TRON in the original versions of the novice-oriented BASIC programming language. TRON stood for, \"Trace On.\" TRON caused the line numbers of each BASIC command line to print as the program ran.\\n \\'\\'Remote debugging\\'\\' is the process of debugging a program running on a system different from the debugger. To start remote debugging, a debugger connects to a remote system over a network. The debugger can then control the execution of the program on the remote system and retrieve information about its state.\\n \\'\\'Post-mortem debugging\\'\\' is debugging of the program after it has already Crash (computing)|crashed. Related techniques often include various tracing techniques (for example,[http://www.drdobbs.com/tools/185300443 Postmortem Debugging, Stephen Wormuller, Dr. Dobbs Journal, 2006]) and/or analysis of memory dump (or core dump) of the crashed process. The dump of the process could be obtained automatically by the system (for example, when process has terminated due to an unhandled exception), or by a programmer-inserted instruction, or manually by the interactive user.\\n \\'\\'\"Wolf fence\" algorithm:\\'\\' Edward Gauss described this simple but very useful and now famous algorithm in a 1982 article for communications of the ACM as follows: \"There\\'s one wolf in Alaska; how do you find it? First build a fence down the middle of the state, wait for the wolf to howl, determine which side of the fence it is on. Repeat process on that side only, until you get to the point where you can see the wolf.\"<ref name=\"communications of the ACM\">{{cite journal | title=\"Pracniques: The \"Wolf Fence\" Algorithm for Debugging\", | author=E. J. Gauss | year=1982}} This is implemented e.g. in the Git (software)|Git version control system as the command \\'\\'git bisect\\'\\', which uses the above algorithm to determine which Commit (data management)|commit introduced a particular bug.\\n \\'\\'Delta Debugging\\'\\'{{snd}} a technique of automating test case simplification.Andreas Zeller: <cite>Why Programs Fail: A Guide to Systematic Debugging</cite>, Morgan Kaufmann, 2005. ISBN 1-55860-866-4{{rp|p.123}}<!-- for redirect from \\'Saff Squeeze\\' -->\\n \\'\\'Saff Squeeze\\'\\'{{snd}} a technique of isolating failure within the test using progressive inlining of parts of the failing test.[http://www.threeriversinstitute.org/HitEmHighHitEmLow.html Kent Beck, Hit \\'em High, Hit \\'em Low: Regression Testing and the Saff Squeeze]\\n\\nDebugging for embedded systems\\nIn contrast to the general purpose computer software design environment, a primary characteristic of embedded environments is the sheer number of different platforms available to the developers (CPU architectures, vendors, operating systems and their variants). Embedded systems are, by definition, not general-purpose designs: they are typically developed for a single task (or small range of tasks), and the platform is chosen specifically to optimize that application. Not only does this fact make life tough for embedded system developers, it also makes debugging and testing of these systems harder as well, since different debugging tools are needed in different platforms.\\n\\nto identify and fix bugs in the system (e.g. logical or synchronization problems in the code, or a design error in the hardware);\\nto collect information about the operating states of the system that may then be used to analyze the system: to find ways to boost its performance or to optimize other important characteristics (e.g. energy consumption, reliability, real-time response etc.).\\n\\nAnti-debugging\\nAnti-debugging is \"the implementation of one or more techniques within computer code that hinders attempts at reverse engineering or debugging a target process\".<ref name=\"veracode-antidebugging\">{{cite web |url=http://www.veracode.com/blog/2008/12/anti-debugging-series-part-i/ |title=Anti-Debugging Series - Part I |last=Shields |first=Tyler |date=2008-12-02 |work=Veracode |accessdate=2009-03-17}} It is actively used by recognized publishers in copy protection|copy-protection schemas, but is also used by malware to complicate its detection and elimination.<ref name=\"soft-prot\">[http://people.seas.harvard.edu/~mgagnon/software_protection_through_anti_debugging.pdf Software Protection through Anti-Debugging Michael N Gagnon, Stephen Taylor, Anup Ghosh] Techniques used in anti-debugging include:\\nAPI-based: check for the existence of a debugger using system information\\nException-based: check to see if exceptions are interfered with\\nProcess and thread blocks: check whether process and thread blocks have been manipulated\\nModified code: check for code modifications made by a debugger handling software breakpoints\\nHardware- and register-based: check for hardware breakpoints and CPU registers\\nTiming and latency: check the time taken for the execution of instructions\\nDetecting and penalizing debugger<ref name=\"soft-prot\" /><!-- reference does not exist -->\\n\\nAn early example of anti-debugging existed in early versions of Microsoft Word which, if a debugger was detected, produced a message that said: \"The tree of evil bears bitter fruit. Now trashing program disk.\", after which it caused the floppy disk drive to emit alarming noises with the intent of scaring the user away from attempting it again.<ref name=\"SecurityEngineeringRA\">{{cite book | url=http://www.cl.cam.ac.uk/~rja14/book.html | author=Ross J. Anderson | title=Security Engineering | isbn = 0-471-38922-6 | page=684 }}<ref name=\"toastytech\">{{cite web | url=http://toastytech.com/guis/word1153.html | title=Microsoft Word for DOS 1.15}}\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB20I26xW963",
        "outputId": "9af644a1-12c7-475f-d124-88c17af530f3"
      },
      "source": [
        "# Import Counter\n",
        "from collections import Counter\n",
        "\n",
        "# Tokenize the article: tokens\n",
        "tokens = word_tokenize(article)\n",
        "\n",
        "# Convert the tokens into lowercase: lower_tokens\n",
        "lower_tokens = [t.lower() for t in tokens]\n",
        "\n",
        "# Create a Counter with the lowercase tokens: bow_simple\n",
        "bow_simple = Counter(lower_tokens)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow_simple.most_common(10))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 68), ('to', 63), ('a', 60), ('in', 44), ('and', 41), ('debugging', 40)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um2-Hrz_X97Z"
      },
      "source": [
        "## Text preprocessing practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvYDkFjPX-hR"
      },
      "source": [
        "english_stops = ['i',\n",
        " 'me',\n",
        " 'my',\n",
        " 'myself',\n",
        " 'we',\n",
        " 'our',\n",
        " 'ours',\n",
        " 'ourselves',\n",
        " 'you',\n",
        " 'your',\n",
        " 'yours',\n",
        " 'yourself',\n",
        " 'yourselves',\n",
        " 'he',\n",
        " 'him',\n",
        " 'his',\n",
        " 'himself',\n",
        " 'she',\n",
        " 'her',\n",
        " 'hers',\n",
        " 'herself',\n",
        " 'it',\n",
        " 'its',\n",
        " 'itself',\n",
        " 'they',\n",
        " 'them',\n",
        " 'their',\n",
        " 'theirs',\n",
        " 'themselves',\n",
        " 'what',\n",
        " 'which',\n",
        " 'who',\n",
        " 'whom',\n",
        " 'this',\n",
        " 'that',\n",
        " 'these',\n",
        " 'those',\n",
        " 'am',\n",
        " 'is',\n",
        " 'are',\n",
        " 'was',\n",
        " 'were',\n",
        " 'be',\n",
        " 'been',\n",
        " 'being',\n",
        " 'have',\n",
        " 'has',\n",
        " 'had',\n",
        " 'having',\n",
        " 'do',\n",
        " 'does',\n",
        " 'did',\n",
        " 'doing',\n",
        " 'a',\n",
        " 'an',\n",
        " 'the',\n",
        " 'and',\n",
        " 'but',\n",
        " 'if',\n",
        " 'or',\n",
        " 'because',\n",
        " 'as',\n",
        " 'until',\n",
        " 'while',\n",
        " 'of',\n",
        " 'at',\n",
        " 'by',\n",
        " 'for',\n",
        " 'with',\n",
        " 'about',\n",
        " 'against',\n",
        " 'between',\n",
        " 'into',\n",
        " 'through',\n",
        " 'during',\n",
        " 'before',\n",
        " 'after',\n",
        " 'above',\n",
        " 'below',\n",
        " 'to',\n",
        " 'from',\n",
        " 'up',\n",
        " 'down',\n",
        " 'in',\n",
        " 'out',\n",
        " 'on',\n",
        " 'off',\n",
        " 'over',\n",
        " 'under',\n",
        " 'again',\n",
        " 'further',\n",
        " 'then',\n",
        " 'once',\n",
        " 'here',\n",
        " 'there',\n",
        " 'when',\n",
        " 'where',\n",
        " 'why',\n",
        " 'how',\n",
        " 'all',\n",
        " 'any',\n",
        " 'both',\n",
        " 'each',\n",
        " 'few',\n",
        " 'more',\n",
        " 'most',\n",
        " 'other',\n",
        " 'some',\n",
        " 'such',\n",
        " 'no',\n",
        " 'nor',\n",
        " 'not',\n",
        " 'only',\n",
        " 'own',\n",
        " 'same',\n",
        " 'so',\n",
        " 'than',\n",
        " 'too',\n",
        " 'very',\n",
        " 's',\n",
        " 't',\n",
        " 'can',\n",
        " 'will',\n",
        " 'just',\n",
        " 'don',\n",
        " 'should',\n",
        " 'now',\n",
        " 'd',\n",
        " 'll',\n",
        " 'm',\n",
        " 'o',\n",
        " 're',\n",
        " 've',\n",
        " 'y',\n",
        " 'ain',\n",
        " 'aren',\n",
        " 'couldn',\n",
        " 'didn',\n",
        " 'doesn',\n",
        " 'hadn',\n",
        " 'hasn',\n",
        " 'haven',\n",
        " 'isn',\n",
        " 'ma',\n",
        " 'mightn',\n",
        " 'mustn',\n",
        " 'needn',\n",
        " 'shan',\n",
        " 'shouldn',\n",
        " 'wasn',\n",
        " 'weren',\n",
        " 'won',\n",
        " 'wouldn',\n",
        " '']\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs1pt3-hY6B3",
        "outputId": "1a88591c-74a4-40a6-868e-825f9ae20bf1"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "# Import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Retain alphabetic words: alpha_only\n",
        "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
        "\n",
        "# Remove all stop words: no_stops\n",
        "no_stops = [t for t in alpha_only if t not in english_stops]\n",
        "\n",
        "# Instantiate the WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize all tokens into a new list: lemmatized\n",
        "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
        "\n",
        "# Create the bag-of-words: bow\n",
        "bow = Counter(lemmatized)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow.most_common(10))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[('debugging', 40), ('system', 25), ('software', 16), ('bug', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('used', 12)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRILKLT4L5dz"
      },
      "source": [
        "# Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqWYyf82L9vp"
      },
      "source": [
        "## NER with NLTK\n",
        "You're now going to have some fun with named-entity recognition! A scraped news article has been pre-loaded into your workspace. Your task is to use nltk to find the named entities in this article.\n",
        "\n",
        "What might the article be about, given the names you found?\n",
        "\n",
        "Along with nltk, sent_tokenize and word_tokenize from nltk.tokenize have been pre-imported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1wQRM15L_b0"
      },
      "source": [
        "article = '\\ufeffThe taxi-hailing company Uber brings into very sharp focus the question of whether corporations can be said to have a moral character. If any human being were to behave with the single-minded and ruthless greed of the company, we would consider them sociopathic. Uber wanted to know as much as possible about the people who use its service, and those who don’t. It has an arrangement with unroll.me, a company which offered a free service for unsubscribing from junk mail, to buy the contacts unroll.me customers had had with rival taxi companies. Even if their email was notionally anonymised, this use of it was not something the users had bargained for. Beyond that, it keeps track of the phones that have been used to summon its services even after the original owner has sold them, attempting this with Apple’s phones even thought it is forbidden by the company.\\r\\n\\r\\n\\r\\nUber has also tweaked its software so that regulatory agencies that the company regarded as hostile would, when they tried to hire a driver, be given false reports about the location of its cars. Uber management booked and then cancelled rides with a rival taxi-hailing company which took their vehicles out of circulation. Uber deny this was the intention. The punishment for this behaviour was negligible. Uber promised not to use this “greyball” software against law enforcement – one wonders what would happen to someone carrying a knife who promised never to stab a policeman with it. Travis Kalanick of Uber got a personal dressing down from Tim Cook, who runs Apple, but the company did not prohibit the use of the app. Too much money was at stake for that.\\r\\n\\r\\n\\r\\nMillions of people around the world value the cheapness and convenience of Uber’s rides too much to care about the lack of drivers’ rights or pay. Many of the users themselves are not much richer than the drivers. The “sharing economy” encourages the insecure and exploited to exploit others equally insecure to the profit of a tiny clique of billionaires. Silicon Valley’s culture seems hostile to humane and democratic values. The outgoing CEO of Yahoo, Marissa Mayer, who is widely judged to have been a failure, is likely to get a $186m payout. This may not be a cause for panic, any more than the previous hero worship should have been a cause for euphoria. Yet there’s an urgent political task to tame these companies, to ensure they are punished when they break the law, that they pay their taxes fairly and that they behave responsibly.'"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3haEel4rMwkL",
        "outputId": "c2c2e59f-f3fb-4271-da2d-917552311c1f"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAFAR_FXMmu5",
        "outputId": "93602a87-b4ae-4d93-b5e2-383b0bc95b75"
      },
      "source": [
        "# Tokenize the article into sentences: sentences\n",
        "sentences = sent_tokenize(article)\n",
        "\n",
        "# Tokenize each sentence into words: token_sentences\n",
        "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
        "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
        "\n",
        "# Create the named entity chunks: chunked_sentences\n",
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences,binary=True)\n",
        "\n",
        "# Test for stems of the tree with 'NE' tags\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
        "            print(chunk)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(NE Uber/NNP)\n",
            "(NE Beyond/NN)\n",
            "(NE Apple/NNP)\n",
            "(NE Uber/NNP)\n",
            "(NE Uber/NNP)\n",
            "(NE Travis/NNP Kalanick/NNP)\n",
            "(NE Tim/NNP Cook/NNP)\n",
            "(NE Apple/NNP)\n",
            "(NE Silicon/NNP Valley/NNP)\n",
            "(NE CEO/NNP)\n",
            "(NE Yahoo/NNP)\n",
            "(NE Marissa/NNP Mayer/NNP)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgb78r6_NYie"
      },
      "source": [
        "## Charting practice\n",
        "In this exercise, you'll use some extracted named entities and their groupings from a series of newspaper articles to chart the diversity of named entity types in the articles.\n",
        "\n",
        "You'll use a defaultdict called ner_categories, with keys representing every named entity group type, and values to count the number of each different named entity type. You have a chunked sentence list called chunked_sentences similar to the last exercise, but this time with non-binary category names.\n",
        "\n",
        "You can use hasattr() to determine if each chunk has a 'label' and then simply use the chunk's .label() method as the dictionary key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL2iudMGOPj2",
        "outputId": "3f621da1-1ff1-4bf8-92c3-1731874e7bc1"
      },
      "source": [
        "chunked_sentences"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object ParserI.parse_sents.<locals>.<genexpr> at 0x7f5da857ca50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TX6S-VfNggJ"
      },
      "source": [
        "from collections import defaultdict"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "l7Z1csRVNY6V",
        "outputId": "dc37881b-d0aa-4aef-cefc-d80f166d23cf"
      },
      "source": [
        "# Create the named entity chunks: chunked_sentences\n",
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences)\n",
        "\n",
        "# Create the defaultdict: ner_categories\n",
        "ner_categories = defaultdict(int)\n",
        "\n",
        "# Create the nested for loop\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, 'label'):\n",
        "            ner_categories[chunk.label()] += 1\n",
        "            \n",
        "# Create a list from the dictionary keys for the chart labels: labels\n",
        "labels = list(ner_categories.keys())\n",
        "\n",
        "# Create a list of the values: values\n",
        "values = [ner_categories.get(v) for v in labels]\n",
        "\n",
        "# Create the pie chart\n",
        "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "\n",
        "# Display the chart\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixtRMFOHN_DB",
        "outputId": "16b502a6-e169-4612-cc05-f2d52d591f77"
      },
      "source": [
        "labels"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['GPE', 'PERSON', 'ORGANIZATION']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8b8JirmPlox"
      },
      "source": [
        "## Comparing NLTK with spaCy NER\n",
        "Using the same text you used in the first exercise of this chapter, you'll now see the results using spaCy's NER annotator. How will they compare?\n",
        "\n",
        "The article has been pre-loaded as article. To minimize execution times, you'll be asked to specify the keyword arguments tagger=False, parser=False, matcher=False when loading the spaCy model, because you only care about the entity in this exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT2BOaNWQVyz"
      },
      "source": [
        "article = '\\ufeffThe taxi-hailing company Uber brings into very sharp focus the question of whether corporations can be said to have a moral character. If any human being were to behave with the single-minded and ruthless greed of the company, we would consider them sociopathic. Uber wanted to know as much as possible about the people who use its service, and those who don’t. It has an arrangement with unroll.me, a company which offered a free service for unsubscribing from junk mail, to buy the contacts unroll.me customers had had with rival taxi companies. Even if their email was notionally anonymised, this use of it was not something the users had bargained for. Beyond that, it keeps track of the phones that have been used to summon its services even after the original owner has sold them, attempting this with Apple’s phones even thought it is forbidden by the company.\\r\\n\\r\\n\\r\\nUber has also tweaked its software so that regulatory agencies that the company regarded as hostile would, when they tried to hire a driver, be given false reports about the location of its cars. Uber management booked and then cancelled rides with a rival taxi-hailing company which took their vehicles out of circulation. Uber deny this was the intention. The punishment for this behaviour was negligible. Uber promised not to use this “greyball” software against law enforcement – one wonders what would happen to someone carrying a knife who promised never to stab a policeman with it. Travis Kalanick of Uber got a personal dressing down from Tim Cook, who runs Apple, but the company did not prohibit the use of the app. Too much money was at stake for that.\\r\\n\\r\\n\\r\\nMillions of people around the world value the cheapness and convenience of Uber’s rides too much to care about the lack of drivers’ rights or pay. Many of the users themselves are not much richer than the drivers. The “sharing economy” encourages the insecure and exploited to exploit others equally insecure to the profit of a tiny clique of billionaires. Silicon Valley’s culture seems hostile to humane and democratic values. The outgoing CEO of Yahoo, Marissa Mayer, who is widely judged to have been a failure, is likely to get a $186m payout. This may not be a cause for panic, any more than the previous hero worship should have been a cause for euphoria. Yet there’s an urgent political task to tame these companies, to ensure they are punished when they break the law, that they pay their taxes fairly and that they behave responsibly.'"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMFgM1oqPmRX",
        "outputId": "8792799f-3528-4036-bb7b-83788273f5bd"
      },
      "source": [
        "# Import spacy\n",
        "import spacy\n",
        "\n",
        "# Instantiate the English model: nlp\n",
        "nlp = spacy.load('en',tagger=False,parser=False,matcher=False)\n",
        "\n",
        "# Create a new document: doc\n",
        "doc = nlp(article)\n",
        "\n",
        "# Print all of the found entities and their labels\n",
        "for ent in doc.ents:\n",
        "    print(ent.label_, ent.text)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ORG Uber\n",
            "PERSON Uber\n",
            "ORG unroll.me\n",
            "ORG Apple\n",
            "PERSON Travis Kalanick\n",
            "PERSON Uber\n",
            "PERSON Tim Cook\n",
            "ORG Apple\n",
            "CARDINAL Millions\n",
            "PERSON Uber\n",
            "LOC Silicon Valley\n",
            "ORG Yahoo\n",
            "PERSON Marissa Mayer\n",
            "MONEY $186m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8PzKFZ9RRPT"
      },
      "source": [
        "## French NER with polyglot I\n",
        "In this exercise and the next, you'll use the polyglot library to identify French entities. The library functions slightly differently than spacy, so you'll use a few of the new things you learned in the last video to display the named entity text and category.\n",
        "\n",
        "You have access to the full article string in article. Additionally, the Text class of polyglot has been imported from polyglot.text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJZgMtyDRjFH",
        "outputId": "a0cff8d3-de29-4389-f470-1ffa64eecb7e"
      },
      "source": [
        "!pip install polyglot\n",
        "!pip install PyICU\n",
        "!pip install pycld2"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting polyglot\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/98/e24e2489114c5112b083714277204d92d372f5bbe00d5507acf40370edb9/polyglot-16.7.4.tar.gz (126kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 21.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 81kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 112kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 8.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: polyglot\n",
            "  Building wheel for polyglot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for polyglot: filename=polyglot-16.7.4-py2.py3-none-any.whl size=52557 sha256=26a0b70f5fb1cfb6867fda6765398d3fe676f742c90734ff87487f8a0a4250fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/91/ef/f1369fdc1203b0a9347d4b24f149b83a305f39ab047986d9da\n",
            "Successfully built polyglot\n",
            "Installing collected packages: polyglot\n",
            "Successfully installed polyglot-16.7.4\n",
            "Collecting PyICU\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/0f/9d6b7eb01650960239a5d4dc21cd6e7a96921807c043d287bae4b2f440e1/PyICU-2.7.2.tar.gz (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 8.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyICU\n",
            "  Building wheel for PyICU (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyICU: filename=PyICU-2.7.2-cp37-cp37m-linux_x86_64.whl size=1341216 sha256=4040ed7fffd6169429b4d73e1a245c976c5abb3deca954f74aeb74ccab332369\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/d5/80/bb5bb9071021eff7241fc700124c0af4caadccf2db23e8dcee\n",
            "Successfully built PyICU\n",
            "Installing collected packages: PyICU\n",
            "Successfully installed PyICU-2.7.2\n",
            "Collecting pycld2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/d2/8b0def84a53c88d0eb27c67b05269fbd16ad68df8c78849e7b5d65e6aec3/pycld2-0.41.tar.gz (41.4MB)\n",
            "\u001b[K     |████████████████████████████████| 41.4MB 109kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pycld2\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp37-cp37m-linux_x86_64.whl size=9834216 sha256=c0c692534f107e4b4c99f93e5d08c1859c1d75c484ca88006fa181b70aec3c58\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/8f/e9/08a1a8932a490175bd140206cd86a3dbcfc70498100de11079\n",
            "Successfully built pycld2\n",
            "Installing collected packages: pycld2\n",
            "Successfully installed pycld2-0.41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9GqaRKxSNNF",
        "outputId": "8abff7ca-0c32-46f1-eb19-30dc8bbc59a8"
      },
      "source": [
        "!pip install morfessor"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting morfessor\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Installing collected packages: morfessor\n",
            "Successfully installed morfessor-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmLFOd3KSarf",
        "outputId": "aa419d6e-ef45-4090-9044-049fe812bded"
      },
      "source": [
        "!polyglot download embeddings2.fr\n",
        "!polyglot download ner2.fr"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[polyglot_data] Downloading package embeddings2.fr to\n",
            "[polyglot_data]     /root/polyglot_data...\n",
            "[polyglot_data] Downloading package ner2.fr to /root/polyglot_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAyfu0eORpgg"
      },
      "source": [
        "import polyglot"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcDmB8-1RSj_"
      },
      "source": [
        "from polyglot.text import Text"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME12tQuaRRoc"
      },
      "source": [
        "article = \"\\ufeffédition abonné\\r\\n\\r\\n\\r\\nDans une tribune au « Monde », l’universitaire Charles Cuvelliez estime que le fantasme d’un remplacement de l’homme par l’algorithme et le robot repose sur un malentendu.\\r\\n\\r\\n\\r\\nLe Monde | 10.05.2017 à 06h44 • Mis à jour le 10.05.2017 à 09h47 | Par Charles Cuvelliez (Professeur à l’Ecole polytechnique de l'université libre de Bruxelles)\\r\\n\\r\\n\\r\\nTRIBUNE. L’usage morbide, par certains, de Facebook Live a amené son fondateur à annoncer précipitamment le recrutement de 3 000 modérateurs supplémentaires. Il est vrai que l’intelligence artificielle (IA) est bien en peine de reconnaître des contenus violents, surtout diffusés en direct.\\r\\n\\r\\n\\r\\nLe quotidien affreux de ces modérateurs, contraints de visionner des horreurs à longueur de journée, mériterait pourtant qu’on les remplace vite par des machines !\\r\\n\\r\\n\\r\\nL’IA ne peut pas tout, mais là où elle peut beaucoup, on la maudit, accusée de détruire nos emplois, de remplacer la convivialité humaine. Ce débat repose sur un malentendu.\\r\\n\\r\\n\\r\\nIl vient d’une définition de l’IA qui n’a, dans la réalité, jamais pu être mise en pratique : en 1955, elle était vue comme la création de programmes informatiques qui, quoi qu’on leur confie, le feraient un jour mieux que les humains. On pensait que toute caractéristique de l’intelligence humaine pourrait un jour être si précisément décrite qu’il suffirait d’une machine pour la simuler. Ce n’est pas vrai.\\r\\n\\r\\n\\r\\nAngoisses infondées\\r\\n\\r\\n\\r\\nComme le dit un récent Livre blanc sur la question (Pourquoi il ne faut pas avoir peur de l’Intelligence arti\\xadficielle, Julien Maldonato, Deloitte, mars 2017), rien ne pourra remplacer un humain dans sa globalité.\\r\\n\\r\\n\\r\\nL’IA, c’est de l’apprentissage automatique doté d’un processus d’ajustement de modèles statistiques à des masses de données, explique l’auteur. Il s’agit d’un apprentissage sur des paramètres pour lesquels une vision humaine n’explique pas pourquoi ils marchent si bien dans un contexte donné.\\r\\n\\r\\n\\r\\nC’est aussi ce que dit le rapport de l’Office parlementaire d’évaluation des choix scientifiques et technologiques (« Pour une intelligence artificielle maîtrisée, utile et démystifiée », 29 mars 2017), pour qui ce côté « boîte noire » explique des angoisses infondées. Ethiquement, se fonder sur l’IA pour des tâches critiques sans bien comprendre le comment...\""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s8k4eBVSRk7",
        "outputId": "aeedae31-770a-4218-87dc-5f3edce540b7"
      },
      "source": [
        "# Create a new text object using Polyglot's Text class: txt\n",
        "txt = Text(article)\n",
        "\n",
        "# Print each of the entities found\n",
        "for ent in txt.entities:\n",
        "    print(ent)\n",
        "    \n",
        "# Print the type of ent\n",
        "print(type(ent))\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Charles', 'Cuvelliez']\n",
            "['Charles', 'Cuvelliez']\n",
            "['Bruxelles']\n",
            "['l’IA']\n",
            "['Julien', 'Maldonato']\n",
            "['Deloitte']\n",
            "['Ethiquement']\n",
            "['l’IA']\n",
            "['.']\n",
            "<class 'polyglot.text.Chunk'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp7eJEp6TDME",
        "outputId": "b490503f-fad9-4c07-e359-bca028335ec1"
      },
      "source": [
        "# Create the list of tuples: entities\n",
        "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
        "\n",
        "# Print entities\n",
        "print(entities)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('I-PER', 'Charles Cuvelliez'), ('I-PER', 'Charles Cuvelliez'), ('I-ORG', 'Bruxelles'), ('I-PER', 'l’IA'), ('I-PER', 'Julien Maldonato'), ('I-ORG', 'Deloitte'), ('I-PER', 'Ethiquement'), ('I-LOC', 'l’IA'), ('I-PER', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEhryrd4TKZ8"
      },
      "source": [
        "## Spanish NER with polyglot\n",
        "You'll continue your exploration of polyglot now with some Spanish annotation. This article is not written by a newspaper, so it is your first example of a more blog-like text. How do you think that might compare when finding entities?\n",
        "\n",
        "The Text object has been created as txt, and each entity has been printed, as you can see in the IPython Shell.\n",
        "\n",
        "Your specific task is to determine how many of the entities contain the words \"Márquez\" or \"Gabo\" - these refer to the same person in different ways!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhdbycdtT1x5",
        "outputId": "dfecd38f-0486-47c4-d3c3-eaf8dcf1aff3"
      },
      "source": [
        "!polyglot download embeddings2.es\n",
        "!polyglot download ner2.es"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[polyglot_data] Downloading package embeddings2.es to\n",
            "[polyglot_data]     /root/polyglot_data...\n",
            "[polyglot_data] Downloading package ner2.es to /root/polyglot_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3_y3JnSTK_U"
      },
      "source": [
        "article = \"Lina del Castillo es profesora en el Instituto de Estudios Latinoamericanos Teresa Lozano Long (LLILAS) y el Departamento de Historia de la Universidad de Texas en Austin. Ella será la moderadora del panel “Los Mundos Políticos de Gabriel García Márquez” este viernes, Oct. 30, en el simposio Gabriel García Márquez: Vida y Legado. LIna del Castillo Actualmente, sus investigaciones abarcan la intersección de cartografía, disputas a las demandas de tierra y recursos, y la formación del n...el tren de medianoche que lleva a miles y miles de cadáveres uno encima del otro como tantos racimos del banano que acabarán tirados al mar. Ningún recuento periodístico podría provocar nuestra imaginación y nuestra memoria como este relato de García Márquez. Contenido Relacionado Lea más artículos sobre el archivo de Gabriel García Márquez Reciba mensualmente las últimas noticias e información del Harry Ransom Center con eNews, nuestro correo electrónico mensual. ¡Suscríbase hoy!\""
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwtCJnW9TvhB",
        "outputId": "437a9bad-9048-43d7-8386-0daf13088c6b"
      },
      "source": [
        "txt = Text(article)\n",
        "\n",
        "# Initialize the count variable: count\n",
        "count = 0\n",
        "\n",
        "# Iterate over all the entities\n",
        "for ent in txt.entities:\n",
        "    # Check whether the entity contains 'Márquez' or 'Gabo'\n",
        "    if 'Márquez' in ent or 'Gabo' in ent:\n",
        "        # Increment count\n",
        "        count+=1\n",
        "\n",
        "# Print count\n",
        "print(count)\n",
        "\n",
        "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
        "percentage = count / len(txt.entities)\n",
        "print(percentage)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "0.26666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-8ouFnPpjU7"
      },
      "source": [
        "# Building a \"fake news\" classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvsDEc26qX2n"
      },
      "source": [
        "## CountVectorizer for text classification\n",
        "It's time to begin building your text classifier! The data has been loaded into a DataFrame called df. Explore it in the IPython Shell to investigate what columns you can use. The .head() method is particularly informative.\n",
        "\n",
        "In this exercise, you'll use pandas alongside scikit-learn to create a sparse text vectorizer you can use to train and test a simple supervised model. To begin, you'll set up a CountVectorizer and investigate some of its features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL7vmZ0Ssun6"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd39P1yVpnHt",
        "outputId": "6421e595-440d-4c96-ce0c-3a3fdf1afd2f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugH-FuBqshNh"
      },
      "source": [
        "link = '/content/drive/My Drive/dataset/fake_or_real_news.csv'\n",
        "df = pd.read_csv(link)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsWKU2LOs1VY",
        "outputId": "544b667b-8ff5-4e35-ae75-7de51b5785b9"
      },
      "source": [
        "# Import the necessary modules\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Print the head of df\n",
        "print(df.head())\n",
        "\n",
        "# Create a series to store the labels: y\n",
        "y = df.label\n",
        "\n",
        "# Create training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'],y,test_size = 0.33, random_state = 53)\n",
        "\n",
        "# Initialize a CountVectorizer object: count_vectorizer\n",
        "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
        "\n",
        "# Transform the training data using only the 'text' column values: count_train \n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using only the 'text' column values: count_test \n",
        "count_test = count_vectorizer.transform(X_test)\n",
        "\n",
        "# Print the first 10 features of the count_vectorizer\n",
        "print(count_vectorizer.get_feature_names()[:10])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0  ... label\n",
            "0        8476  ...  FAKE\n",
            "1       10294  ...  FAKE\n",
            "2        3608  ...  REAL\n",
            "3       10142  ...  FAKE\n",
            "4         875  ...  REAL\n",
            "\n",
            "[5 rows x 4 columns]\n",
            "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45BTGNuJtah3"
      },
      "source": [
        "## Using TfidfVectorizer\n",
        "Similar to the sparse CountVectorizer created in the previous exercise, you'll work on creating tf-idf vectors for your documents. You'll set up a TfidfVectorizer and investigate some of its features.\n",
        "\n",
        "In this exercise, you'll use pandas and sklearn along with the same X_train, y_train and X_test, y_test DataFrames and Series you created in the last exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRvRHliZtcDs",
        "outputId": "91728f37-3873-404c-e058-56e60c755d47"
      },
      "source": [
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df = 0.7)\n",
        "\n",
        "# Transform the training data: tfidf_train \n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data: tfidf_test \n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Print the first 10 features\n",
        "print(tfidf_vectorizer.get_feature_names()[:10])\n",
        "\n",
        "# Print the first 5 vectors of the tfidf training data\n",
        "print(tfidf_train.A[:5])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPqfp0litlbB"
      },
      "source": [
        "## Inspecting the vectors\n",
        "To get a better idea of how the vectors work, you'll investigate them by converting them into pandas DataFrames.\n",
        "\n",
        "Here, you'll use the same data structures you created in the previous two exercises (count_train, count_vectorizer, tfidf_train, tfidf_vectorizer) as well as pandas, which is imported as pd."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc2kwSxFtl83",
        "outputId": "ecafc31e-7ec1-4ccd-f9a7-7ed553a823a5"
      },
      "source": [
        "# Create the CountVectorizer DataFrame: count_df\n",
        "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
        "\n",
        "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
        "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
        "\n",
        "# Print the head of count_df\n",
        "print(count_df.head())\n",
        "\n",
        "# Print the head of tfidf_df\n",
        "print(tfidf_df.head())\n",
        "\n",
        "# Calculate the difference in columns: difference\n",
        "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
        "print(difference)\n",
        "\n",
        "# Check whether the DataFrames are equal\n",
        "print(count_df.equals(tfidf_df))\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   00  000  0000  00000031  000035  00006  ...  ما  محاولات  من  هذا  والمرضى  ยงade\n",
            "0   0    0     0         0       0      0  ...   0        0   0    0        0      0\n",
            "1   0    0     0         0       0      0  ...   0        0   0    0        0      0\n",
            "2   0    0     0         0       0      0  ...   0        0   0    0        0      0\n",
            "3   0    0     0         0       0      0  ...   0        0   0    0        0      0\n",
            "4   0    0     0         0       0      0  ...   0        0   0    0        0      0\n",
            "\n",
            "[5 rows x 56922 columns]\n",
            "    00  000  0000  00000031  000035  ...  محاولات   من  هذا  والمرضى  ยงade\n",
            "0  0.0  0.0   0.0       0.0     0.0  ...      0.0  0.0  0.0      0.0    0.0\n",
            "1  0.0  0.0   0.0       0.0     0.0  ...      0.0  0.0  0.0      0.0    0.0\n",
            "2  0.0  0.0   0.0       0.0     0.0  ...      0.0  0.0  0.0      0.0    0.0\n",
            "3  0.0  0.0   0.0       0.0     0.0  ...      0.0  0.0  0.0      0.0    0.0\n",
            "4  0.0  0.0   0.0       0.0     0.0  ...      0.0  0.0  0.0      0.0    0.0\n",
            "\n",
            "[5 rows x 56922 columns]\n",
            "set()\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k5OtesIvV_R"
      },
      "source": [
        "## Training and testing the \"fake news\" model with CountVectorizer\n",
        "Now it's your turn to train the \"fake news\" model using the features you identified and extracted. In this first exercise you'll train and test a Naive Bayes model using the CountVectorizer data.\n",
        "\n",
        "The training and test sets have been created, and count_vectorizer, count_train, and count_test have been computed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wBo-QQbvXBd",
        "outputId": "1bc68288-41e7-4e61-edc8-538942396798"
      },
      "source": [
        "# Import the necessary modules\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "nb_classifier.fit(count_train, y_train)\n",
        "\n",
        "# Create the predicted tags: pred\n",
        "pred = nb_classifier.predict(count_test)\n",
        "\n",
        "# Calculate the accuracy score: score\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "print(score)\n",
        "\n",
        "# Calculate the confusion matrix: cm\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE','REAL'])\n",
        "\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.893352462936394\n",
            "[[ 865  143]\n",
            " [  80 1003]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1deWXNUDwLFL",
        "outputId": "e758f7eb-fb57-4f0d-c50e-43637afc18f0"
      },
      "source": [
        "cm"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 865,  143],\n",
              "       [  80, 1003]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjNr8ELyv5P-"
      },
      "source": [
        "## Training and testing the \"fake news\" model with TfidfVectorizer\n",
        "Now that you have evaluated the model using the CountVectorizer, you'll do the same using the TfidfVectorizer with a Naive Bayes model.\n",
        "\n",
        "The training and test sets have been created, and tfidf_vectorizer, tfidf_train, and tfidf_test have been computed. Additionally, MultinomialNB and metrics have been imported from, respectively, sklearn.naive_bayes and sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v8wszkhv6Bk",
        "outputId": "98d9112b-d403-46bc-8635-18515f921292"
      },
      "source": [
        "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "nb_classifier.fit(tfidf_train,y_train)\n",
        "\n",
        "# Create the predicted tags: pred\n",
        "pred = nb_classifier.predict(tfidf_test)\n",
        "\n",
        "# Calculate the accuracy score: score\n",
        "score = metrics.accuracy_score(y_test,pred)\n",
        "print(score)\n",
        "\n",
        "# Calculate the confusion matrix: cm\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels = ['FAKE','REAL'])\n",
        "print(cm)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8565279770444764\n",
            "[[ 739  269]\n",
            " [  31 1052]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDFMqwtmxKx-"
      },
      "source": [
        "## Improving your model\n",
        "Your job in this exercise is to test a few different alpha levels using the Tfidf vectors to determine if there is a better performing combination.\n",
        "\n",
        "The training and test sets have been created, and tfidf_vectorizer, tfidf_train, and tfidf_test have been computed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwwUaDFOxjO4"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNBM3ZB8xh58",
        "outputId": "cd1e2486-1237-4cfd-82ff-861f3020bb98"
      },
      "source": [
        "# Create the list of alphas: alphas\n",
        "alphas = np.arange(0,1,0.1)\n",
        "\n",
        "# Define train_and_predict()\n",
        "def train_and_predict(alpha):\n",
        "    # Instantiate the classifier: nb_classifier\n",
        "    nb_classifier = MultinomialNB(alpha)\n",
        "    # Fit to the training data\n",
        "    nb_classifier.fit(tfidf_train,y_train)\n",
        "    # Predict the labels: pred\n",
        "    pred = nb_classifier.predict(tfidf_test)\n",
        "    # Compute accuracy: score\n",
        "    score = metrics.accuracy_score(y_test, pred)\n",
        "    return score\n",
        "\n",
        "# Iterate over the alphas and print the corresponding score\n",
        "for alpha in alphas:\n",
        "    print('Alpha: ', alpha)\n",
        "    print('Score: ', train_and_predict(alpha))\n",
        "    print()\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alpha:  0.0\n",
            "Score:  0.8813964610234337\n",
            "\n",
            "Alpha:  0.1\n",
            "Score:  0.8976566236250598\n",
            "\n",
            "Alpha:  0.2\n",
            "Score:  0.8938307030129125\n",
            "\n",
            "Alpha:  0.30000000000000004\n",
            "Score:  0.8900047824007652\n",
            "\n",
            "Alpha:  0.4\n",
            "Score:  0.8857006217120995\n",
            "\n",
            "Alpha:  0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Score:  0.8842659014825442\n",
            "\n",
            "Alpha:  0.6000000000000001\n",
            "Score:  0.874701099952176\n",
            "\n",
            "Alpha:  0.7000000000000001\n",
            "Score:  0.8703969392635102\n",
            "\n",
            "Alpha:  0.8\n",
            "Score:  0.8660927785748446\n",
            "\n",
            "Alpha:  0.9\n",
            "Score:  0.8589191774270684\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO609kFbxqw2"
      },
      "source": [
        "## Inspecting your model\n",
        "Now that you have built a \"fake news\" classifier, you'll investigate what it has learned. You can map the important vector weights back to actual words using some simple inspection techniques.\n",
        "\n",
        "You have your well performing tfidf Naive Bayes classifier available as nb_classifier, and the vectors as tfidf_vectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iTpcRMPxrZJ",
        "outputId": "2687613c-6386-48dd-af4d-60c546b18a7b"
      },
      "source": [
        "# Get the class labels: class_labels\n",
        "class_labels = nb_classifier.classes_\n",
        "\n",
        "# Extract the features: feature_names\n",
        "feature_names = tfidf_vectorizer.get_feature_names()\n",
        "\n",
        "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
        "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
        "\n",
        "# Print the first class label and the top 20 feat_with_weights entries\n",
        "print(class_labels[0], feat_with_weights[:20])\n",
        "\n",
        "# Print the second class label and the bottom 20 feat_with_weights entries\n",
        "print(class_labels[1], feat_with_weights[-20:])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FAKE [(-11.316312804238807, '0000'), (-11.316312804238807, '000035'), (-11.316312804238807, '0001'), (-11.316312804238807, '0001pt'), (-11.316312804238807, '000km'), (-11.316312804238807, '0011'), (-11.316312804238807, '006s'), (-11.316312804238807, '007'), (-11.316312804238807, '007s'), (-11.316312804238807, '008s'), (-11.316312804238807, '0099'), (-11.316312804238807, '00am'), (-11.316312804238807, '00p'), (-11.316312804238807, '00pm'), (-11.316312804238807, '014'), (-11.316312804238807, '015'), (-11.316312804238807, '018'), (-11.316312804238807, '01am'), (-11.316312804238807, '020'), (-11.316312804238807, '023')]\n",
            "REAL [(-7.742481952533027, 'states'), (-7.717550034444668, 'rubio'), (-7.703583809227384, 'voters'), (-7.654774992495461, 'house'), (-7.649398936153309, 'republicans'), (-7.6246184189367, 'bush'), (-7.616556675728881, 'percent'), (-7.545789237823644, 'people'), (-7.516447881078008, 'new'), (-7.448027933291952, 'party'), (-7.411148410203476, 'cruz'), (-7.410910239085596, 'state'), (-7.35748985914622, 'republican'), (-7.33649923948987, 'campaign'), (-7.2854057032685775, 'president'), (-7.2166878130917755, 'sanders'), (-7.108263114902301, 'obama'), (-6.724771332488041, 'clinton'), (-6.5653954389926845, 'said'), (-6.328486029596207, 'trump')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}